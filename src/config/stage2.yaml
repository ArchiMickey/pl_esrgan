batch_size: 16
num_workers: 12
seed: 42
lr: 2e-4
name: null
project: esrgan
ckpt_path: checkpoints/stage_1/last.ckpt

hr_height: 128
hr_width: 128

model:
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  lr: ${lr}
  min_lr: 1e-6
  lr_check_interval: ${trainer.check_val_every_n_epoch}
  lr_decay_factor: 0.5
  lr_decay_patience: 6
  warmup_steps: 0
  channels: 3
  filters: 64
  upscale_factor: 4
  hr_height: ${hr_height}
  hr_width: ${hr_width}
  num_residual_blocks: 23
  lam_adv: 5e-3
  lam_pixel: 1e-2
  loss_g_scale: 1
  loss_d_scale: 200

datamodule:
  batch_size: ${batch_size}
  num_workers: ${num_workers}
  hr_height: ${hr_height}
  hr_width: ${hr_width}
  pathlist_dict:
    train:
      - datasets/DIV2K/DIV2K_train_HR
      - datasets/Flickr2K
      - datasets/OST/train/animal
      - datasets/OST/train/building
      - datasets/OST/train/grass
      - datasets/OST/train/mountain
      - datasets/OST/train/plant
      - datasets/OST/train/sky
      - datasets/OST/train/water
    val:
      - datasets/DIV2K/DIV2K_valid_HR
      - datasets/OST/val

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  name: ${name}
  project: ${project}

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: 1
  log_every_n_steps: 200
  # max_epochs: 100000
  max_steps: 2e6
  val_check_interval: null
  check_val_every_n_epoch: 2
  gradient_clip_val: 10.0
  logger: ${logger}
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelSummary
      max_depth: 5
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: step
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: checkpoints/stage_1
      filename: '{epoch:02d}-{val_loss:.5f}'
      monitor: val_loss
      save_top_k: 3
      mode: min
      save_last: True
      verbose: True